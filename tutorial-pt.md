# Federation Databricks + Google Cloud Storage usando Bucket e Chave JSON

## Resumo

Recentemente precisei integrar o BigQuery com o Databricks para consumir uma base de dados de forma segura e eficiente.  
ApÃ³s alguns testes, optei por utilizar um bucket no Google Cloud Storage (GCS) com autenticaÃ§Ã£o via chave JSON de uma conta de serviÃ§o.  
Documentei aqui todo o processo tanto como referÃªncia pessoal quanto para apoiar quem estiver passando por algo semelhante.

---

## 1 - Criando um bucket

1.1 - Acesse o [Google Cloud Console](https://console.cloud.google.com/?hl=pt-br)  
1.2 - Na barra de pesquisa, procure por "bucket", como mostro na imagem:  
![DescriÃ§Ã£o da imagem](imagens/img1.png)  
1.3 - Ao abrir a tela de buckets, repare que a opÃ§Ã£o "+ Criar" pode estar desativada. SerÃ¡ necessÃ¡rio adicionar uma forma de pagamento:  
![DescriÃ§Ã£o da imagem](imagens/img2.png)  
1.4 - ApÃ³s inserir os dados de faturamento, a opÃ§Ã£o "+ Criar" ficarÃ¡ ativada:  
![DescriÃ§Ã£o da imagem](imagens/img3.png)  
1.5 - Clique em "+ Criar", escolha um nome para o bucket e clique em "Continuar":  
![DescriÃ§Ã£o da imagem](imagens/img4.png)  
1.6 - Nas opÃ§Ãµes de armazenamento, mantive as configuraÃ§Ãµes padrÃ£o (default):  
![DescriÃ§Ã£o da imagem](imagens/img5.png)  
1.7 - Siga com os padrÃµes nas etapas seguintes. No final, clique em "Criar":  
![DescriÃ§Ã£o da imagem](imagens/img6.png)  
![DescriÃ§Ã£o da imagem](imagens/img7.png)  
1.8 - Com o bucket criado, vocÃª verÃ¡ a tela com a opÃ§Ã£o de upload. Como neste caso vamos puxar dados do BigQuery, seguimos para o prÃ³ximo passo:  
![DescriÃ§Ã£o da imagem](imagens/img8.png)

---

## 2 - Exportando a base do BigQuery

2.1 - Acesse a base no BigQuery. A utilizada neste exemplo estÃ¡ neste link: [Base dos Dados â€“ Medicamentos Industrializados](https://basedosdados.org/dataset/bd52ab08-9980-4831-a88c-a1ac5226ef27?table=26d8e34b-731c-4852-a838-f3f6409a07f6)  
Clique nos trÃªs pontinhos e selecione "Fazer consulta" (Query):  
![DescriÃ§Ã£o da imagem](imagens/img9.png)  
2.2 - FaÃ§a um SELECT simples para entender a estrutura da tabela:  
![DescriÃ§Ã£o da imagem](imagens/img10.png)  
2.3 - No seu projeto, clique nos trÃªs pontinhos e selecione "Criar conjunto de dados":  
![DescriÃ§Ã£o da imagem](imagens/img12.png)  
2.4 - Escolha um nome e clique em "Criar conjunto de dados":  
![DescriÃ§Ã£o da imagem](imagens/img13.png)  
2.5 - Volte Ã  query e crie uma nova tabela copiando os dados da original. Exemplo:  
[**copiar_tabela_microdados.sql**](copy_microdata_table.sql)  
![DescriÃ§Ã£o da imagem](imagens/img14.png)  
2.6 - No canto superior direito da interface, clique no Ã­cone de terminal (Cloud Shell). Use o seguinte comando para extrair a tabela para seu bucket:

```bash
bq extract \
  --destination_format=CSV \
  --field_delimiter="," \
  --print_header=true \
  'databricks-gcs-paula-henriques:anvisa_dados.microdados' \
  'gs://bucket_databrciks_gcs/microdados_anvisa/microdados_*.csv'
```


ApÃ³s a execuÃ§Ã£o, os arquivos devem aparecer conforme a imagem:  
![DescriÃ§Ã£o da imagem](imagens/img15.png)  
2.7 - Para conferir, acesse novamente o bucket via console:  
![DescriÃ§Ã£o da imagem](imagens/img16.png)

---

## 3 - Criando a conta de serviÃ§o

3.1 - Pesquise por "IAM" no console e selecione a opÃ§Ã£o:  
![DescriÃ§Ã£o da imagem](imagens/img21.png)  
3.2 - A tela de IAM serÃ¡ semelhante a esta:  
![DescriÃ§Ã£o da imagem](imagens/img22.png)  
3.3 - No menu lateral, selecione "Contas de serviÃ§o":  
![DescriÃ§Ã£o da imagem](imagens/img23.png)  
3.4 - Clique em "Criar conta de serviÃ§o", escolha um nome e avance:  
![DescriÃ§Ã£o da imagem](imagens/img24.png)  
3.5 - Nas permissÃµes, vÃ¡ em "Em uso" e selecione "ProprietÃ¡rio":  
![DescriÃ§Ã£o da imagem](imagens/img25.png)  
3.6 - Clique em "Continuar":  
![DescriÃ§Ã£o da imagem](imagens/img26.png)  
3.7 - Mantenha o padrÃ£o em "Principais com acesso" e conclua:
![DescriÃ§Ã£o da imagem](imagens/img27.png)

---

## 4 - Gerando a chave de autenticaÃ§Ã£o

4.1 - Com a conta criada, clique sobre ela:
![DescriÃ§Ã£o da imagem](imagens/img28.png)  
4.2 - VÃ¡ atÃ© a aba "Chaves":
![DescriÃ§Ã£o da imagem](imagens/img29.png)  
4.3 - Clique em "Adicionar chave" e selecione "Criar nova chave":  
![DescriÃ§Ã£o da imagem](imagens/img31.png)  
4.4 - Escolha o formato **JSON**:
![DescriÃ§Ã£o da imagem](imagens/img32.png)  
4.5 - O download serÃ¡ feito automaticamente para seu computador:  
![DescriÃ§Ã£o da imagem](imagens/img33.png)  
4.6 - Abra o arquivo .json e copie todo o conteÃºdo. Iremos colar no Databricks:  
![DescriÃ§Ã£o da imagem](imagens/img34.png)

---

## 5 - Conectando ao Databricks

> âš ï¸ Importante: vocÃª precisa ter permissÃµes para criar clusters e conexÃµes no Databricks.

5.1 - Acesse o Databricks, vÃ¡ atÃ© **SQL Warehouse**, escolha o cluster desejado e ligue-o:  
![DescriÃ§Ã£o da imagem](imagens/img17.png)  
5.2 - VÃ¡ atÃ© **CatÃ¡logo**, clique em â€œ+â€ e selecione â€œCreate a connectionâ€:  
![DescriÃ§Ã£o da imagem](imagens/img19.png)  
5.3 - Escolha â€œGoogle BigQueryâ€ como tipo de conexÃ£o e clique em â€œNextâ€:  
![DescriÃ§Ã£o da imagem](imagens/img20.png)  
5.4 - No campo de chave, cole o conteÃºdo do JSON gerado no passo 4.6.  
O project_id tambÃ©m estÃ¡ dentro do JSON. ApÃ³s preencher, clique em â€œCreate a connectionâ€:  
![DescriÃ§Ã£o da imagem](imagens/img35.png)  
5.5 - DÃª um nome para o seu catÃ¡logo, insira novamente o project_id e clique em â€œTest connectionâ€:  
![DescriÃ§Ã£o da imagem](imagens/img36.png)  
5.6 - SerÃ¡ solicitado que selecione o cluster. Lembre-se: ele deve estar ligado. Depois, clique em â€œTestâ€:  
![DescriÃ§Ã£o da imagem](imagens/img37.png)  
5.7 - Se a conexÃ£o for bem-sucedida, vocÃª verÃ¡ uma tela como esta:  
![DescriÃ§Ã£o da imagem](imagens/img38.png)  
5.8 - Nas etapas seguintes (â€œAccessâ€ e â€œKeysâ€), apenas avance clicando em â€œNextâ€:  
![DescriÃ§Ã£o da imagem](imagens/img39.png)  
![DescriÃ§Ã£o da imagem](imagens/img40.png)  
Pronto! ConexÃ£o feita com sucesso. ğŸ˜‰

---

## 6 - ValidaÃ§Ã£o

6.1 - Acesse o **CatÃ¡logo** no Databricks para conferir se a conexÃ£o foi criada corretamente:  
![DescriÃ§Ã£o da imagem](imagens/img41.png)  
6.2 - Como o objetivo era garantir a carga completa da base, realizei um COUNT(*) tanto no Databricks quanto no BigQuery para comparar os totais:  
![DescriÃ§Ã£o da imagem](imagens/img42.png)  
![DescriÃ§Ã£o da imagem](imagens/img43.png)

---

## ObservaÃ§Ãµes finais

Espero que este conteÃºdo tenha te ajudado a entender melhor o processo de integraÃ§Ã£o entre o BigQuery e o Databricks via Google Cloud Storage.  
A ideia aqui foi documentar de forma prÃ¡tica os principais passos que enfrentei, pensando tanto como referÃªncia futura quanto para apoiar quem estiver passando por algo semelhante.

Se vocÃª tiver sugestÃµes, dÃºvidas ou quiser trocar experiÃªncias sobre esse tema, fique Ã  vontade para entrar em contato.
